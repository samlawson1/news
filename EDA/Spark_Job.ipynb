{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nyt_article_search import JSONParse\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "from itertools import chain\n",
    "from spark_job_functions import *\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load JSONs and transform in to lists of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists that the data from each json response will be added to\n",
    "article_ids = []\n",
    "fact_data = []\n",
    "author_data = []\n",
    "subject_data = []\n",
    "people_data = []\n",
    "org_data = []\n",
    "loc_data = []\n",
    "# Big text will be a dictionary where we'll add headline, lead paragraph,\n",
    "# abstract and web url\n",
    "big_text = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'DATA'\n",
    "state = 'TEXAS'\n",
    "# states = [f for f in os.listdir(data_folder) if f != '_Archive']\n",
    "state_folder = os.path.join(data_folder, state)\n",
    "json_files = os.listdir(state_folder)\n",
    "for file in json_files:\n",
    "    filepath = os.path.join(state_folder, file)\n",
    "    json_file = open(filepath, 'r')\n",
    "    data = json.load(json_file)\n",
    "    responses = data['response']['docs']\n",
    "    # Parse each json response in the json file\n",
    "    for respsonse in responses:\n",
    "        j = JSONParse(respsonse)\n",
    "        article_ids.append(j.article_id)\n",
    "        fact_data.append(j.get_article_facts())\n",
    "        extend_list(author_data, j.get_article_authors())\n",
    "        extend_list(subject_data, j.search_article_keywords('subject'))\n",
    "        extend_list(people_data, j.search_article_keywords('persons'))\n",
    "        extend_list(org_data, j.search_article_keywords('organizations'))\n",
    "        extend_list(loc_data, j.search_article_keywords('glocations'))\n",
    "        id_text_dict = {\n",
    "            j.article_id:{\n",
    "                            'headline':j.get_text('headline')[1],\n",
    "                            'abstract':j.get_text('abstract')[1],\n",
    "                            'lead_paragraph':j.get_text('lead_paragraph')[1],\n",
    "                            'web_url':j.get_text('web_url')[1]\n",
    "                        }\n",
    "            }\n",
    "        big_text.update(id_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName('NYT_JSON_ETL').\\\n",
    "        master('local[1]').\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For each result get the column headers for the resulting\n",
    "# #dataframe\n",
    "# def get_table_headers(table):\n",
    "\n",
    "#     table_col_dict = {\n",
    "#         'facts':['article_id', 'publication_date',\n",
    "#                             'word_count', 'total_keywords',\n",
    "#                             'total_authors', 'words_in_headline',\n",
    "#                             'in_print', 'print_page', 'print_section',\n",
    "#                             'news_desk', 'section_name', 'article_type'],\n",
    "#         'authors':['article_id', 'rank', 'role',\n",
    "#                             'firstname', 'middlename', 'lastname', 'qualifier'],\n",
    "#         'subjects':['article_id', 'rank', 'name', 'value', 'major'],\n",
    "#         'text':['article_id', 'text']\n",
    "#     }\n",
    "\n",
    "#     headers = table_col_dict.get(table)\n",
    "#     return(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\1saml\\\\AppData\\\\Local\\\\Temp\\\\spark-6d220d61-7512-49a5-b1a4-f20c1486129f\\\\pyspark-6218f323-9b96-4bee-b90e-33ec6ad7fabc\\\\tmpvvatmjsg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Create spark dfs\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m facts \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfact_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_table_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfacts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m authors \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(author_data, schema\u001b[38;5;241m=\u001b[39mget_table_headers(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthors\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      4\u001b[0m subjects \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(subject_data, schema\u001b[38;5;241m=\u001b[39mget_table_headers(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubjects\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\pyspark\\sql\\session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m   1442\u001b[0m     )\n\u001b[1;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\pyspark\\sql\\session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\pyspark\\sql\\session.py:1116\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;66;03m# convert python objects to sql data\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m internal_data \u001b[38;5;241m=\u001b[39m [struct\u001b[38;5;241m.\u001b[39mtoInternal(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tupled_data]\n\u001b[1;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minternal_data\u001b[49m\u001b[43m)\u001b[49m, struct\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\pyspark\\context.py:824\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[1;34m(self, c, numSlices)\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonParallelizeServer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), numSlices)\n\u001b[1;32m--> 824\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialize_to_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreateRDDServer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RDD(jrdd, \u001b[38;5;28mself\u001b[39m, serializer)\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\pyspark\\context.py:864\u001b[0m, in \u001b[0;36mSparkContext._serialize_to_jvm\u001b[1;34m(self, data, serializer, reader_func, server_func)\u001b[0m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;66;03m# without encryption, we serialize to a file, and we read the file in java and\u001b[39;00m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;66;03m# parallelize from there.\u001b[39;00m\n\u001b[1;32m--> 864\u001b[0m     tempFile \u001b[38;5;241m=\u001b[39m \u001b[43mNamedTemporaryFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelete\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_temp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\tempfile.py:559\u001b[0m, in \u001b[0;36mNamedTemporaryFile\u001b[1;34m(mode, buffering, encoding, newline, suffix, prefix, dir, delete, errors)\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fd\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 559\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43m_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mopener\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopener\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    563\u001b[0m         raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffer\u001b[39m\u001b[38;5;124m'\u001b[39m, file)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\tempfile.py:556\u001b[0m, in \u001b[0;36mNamedTemporaryFile.<locals>.opener\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopener\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m name\n\u001b[1;32m--> 556\u001b[0m     fd, name \u001b[38;5;241m=\u001b[39m \u001b[43m_mkstemp_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fd\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\tempfile.py:256\u001b[0m, in \u001b[0;36m_mkstemp_inner\u001b[1;34m(dir, pre, suf, flags, output_type)\u001b[0m\n\u001b[0;32m    254\u001b[0m _sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtempfile.mkstemp\u001b[39m\u001b[38;5;124m\"\u001b[39m, file)\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43m_os\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0o600\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m    \u001b[38;5;66;03m# try again\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\1saml\\\\AppData\\\\Local\\\\Temp\\\\spark-6d220d61-7512-49a5-b1a4-f20c1486129f\\\\pyspark-6218f323-9b96-4bee-b90e-33ec6ad7fabc\\\\tmpvvatmjsg'"
     ]
    }
   ],
   "source": [
    "#Create spark dfs\n",
    "facts = spark.createDataFrame(fact_data, schema=get_table_headers('facts'))\n",
    "authors = spark.createDataFrame(author_data, schema=get_table_headers('authors'))\n",
    "subjects = spark.createDataFrame(subject_data, schema=get_table_headers('subjects'))\n",
    "people = spark.createDataFrame(people_data, schema=get_table_headers('subjects'))\n",
    "organizations = spark.createDataFrame(org_data, schema=get_table_headers('subjects'))\n",
    "locations = spark.createDataFrame(loc_data, schema=get_table_headers('subjects'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Primary Keys in tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to create an interger primary key for the article_ids\n",
    "ids = [(e + 1000, i) for e, i in enumerate(article_ids)]\n",
    "# Create spark df out of id list\n",
    "# Integer Primary Key is fact_id\n",
    "id_schema = StructType([\n",
    "    StructField('fact_id', IntegerType(), False),\n",
    "    StructField('article_id', StringType(), False)\n",
    "    ])\n",
    "\n",
    "### THIS SHOULD NOT CHANGE ANY FURTHER\n",
    "id_df = spark.createDataFrame(ids, schema=id_schema)\n",
    "\n",
    "\n",
    "\n",
    "#Merge the id_df dataframe into the existing frames\n",
    "# To put fact_id in all the other tables\n",
    "facts = id_df.join(facts, ['article_id'], how = 'inner').drop('article_id')\n",
    "authors = id_df.join(authors, ['article_id'], how = 'inner').drop('article_id')\n",
    "subjects = id_df.join(subjects, ['article_id'], how = 'inner').drop('article_id')\n",
    "people = id_df.join(people, ['article_id'], how = 'inner').drop('article_id')\n",
    "organizations = id_df.join(organizations, ['article_id'], how = 'inner').drop('article_id')\n",
    "locations = id_df.join(locations, ['article_id'], how = 'inner').drop('article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union the subjects, organizations, and locations dfs togehter\n",
    "places_and_things = subjects.union(organizations).union(locations).orderBy(['fact_id', 'rank'])\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+-----+--------+\n",
      "|fact_id|rank|      name| value|major|table_id|\n",
      "+-------+----+----------+------+-----+--------+\n",
      "|   1000|   1|glocations| Texas|    N|1000.001|\n",
      "|   1000|   3|   subject|Ethics|    N|1000.002|\n",
      "+-------+----+----------+------+-----+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+----+--------+---------+----------+--------+---------+--------+\n",
      "|fact_id|rank|    role|firstname|middlename|lastname|qualifier|table_id|\n",
      "+-------+----+--------+---------+----------+--------+---------+--------+\n",
      "|   1000|   1|reported|    Steve|    Barnes|     NYT|     NULL|1000.001|\n",
      "|   1003|   1|reported|  Michael|      NULL|    NULL|     NULL|1003.001|\n",
      "+-------+----+--------+---------+----------+--------+---------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+----------------+-----+--------+\n",
      "|fact_id|rank|   name|           value|major|table_id|\n",
      "+-------+----+-------+----------------+-----+--------+\n",
      "|   1000|   2|persons|ROACH, RICHARD J|    N|1000.001|\n",
      "|   1001|   5|persons| Trump, Donald J|    N|1001.001|\n",
      "+-------+----+-------+----------------+-----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "places_and_things = create_primary_key(places_and_things, 'table_id', 'fact_id', 'rank')\n",
    "authors = create_primary_key(authors, 'table_id', 'fact_id', 'rank')\n",
    "people = create_primary_key(people, 'table_id', 'fact_id', 'rank')\n",
    "places_and_things.show(2)\n",
    "authors.show(2)\n",
    "people.show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Text standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dimensional news desk, section name, and article type files\n",
    "dim_folder = os.path.join('DATA', 'DIM_TABLES')\n",
    "dim_news_desk = os.path.join(dim_folder, 'news_desks.csv')\n",
    "dim_types = os.path.join(dim_folder, 'article_types.csv')\n",
    "dim_sections = os.path.join(dim_folder, 'section_names.csv')\n",
    "\n",
    "news_desk_df = spark.read.option('header', True).csv(dim_news_desk)\n",
    "material_df = spark.read.option('header', True).csv(dim_types)\n",
    "section_df = spark.read.option('header', True).csv(dim_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+----------+--------------+-------------+-----------------+--------+----------+-------------+---------+------------+-------------+\n",
      "|fact_id|publication_date|word_count|total_keywords|total_authors|words_in_headline|in_print|print_page|print_section|news_desk|section_name| article_type|\n",
      "+-------+----------------+----------+--------------+-------------+-----------------+--------+----------+-------------+---------+------------+-------------+\n",
      "|   1083|      2015-11-13|       564|             5|            1|                8|    true|         4|           TR|   Travel|      Travel|         News|\n",
      "|   1062|      2010-04-08|       853|             6|            1|                7|    true|        21|            A| National|     Science|         News|\n",
      "|   1005|      2000-11-12|      1003|             6|            1|               11|    true|        22|            1| National|        U.S.|         News|\n",
      "|   1039|      2005-09-29|      1133|            18|            1|                9|    true|         1|            A| National|        U.S.|News Analysis|\n",
      "|   1040|      2015-04-11|      1044|             8|            1|               10|    true|         9|            A| National|        U.S.|         News|\n",
      "+-------+----------------+----------+--------------+-------------+-----------------+--------+----------+-------------+---------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#standardize any text in the article_type, news_desk, and section_name\n",
    "#columns that aren't in the dimensinal tables\n",
    "#EX: National Desk vs National - should be National\n",
    "facts = standardize_text(facts, 'article_type', material_df)\n",
    "facts = standardize_text(facts, 'news_desk', news_desk_df)\n",
    "facts = standardize_text(facts, 'section_name', section_df)\n",
    "facts.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+----------+--------------+-------------+-----------------+--------+----------+-------------+---------+------------+-------------+\n",
      "|fact_id|publication_date|word_count|total_keywords|total_authors|words_in_headline|in_print|print_page|print_section|news_desk|section_name| article_type|\n",
      "+-------+----------------+----------+--------------+-------------+-----------------+--------+----------+-------------+---------+------------+-------------+\n",
      "|   1083|      2015-11-13|       564|             5|            1|                8|    true|       4.0|           TR|   Travel|      Travel|         News|\n",
      "|   1062|      2010-04-08|       853|             6|            1|                7|    true|      21.0|            A| National|     Science|         News|\n",
      "|   1005|      2000-11-12|      1003|             6|            1|               11|    true|      22.0|            1| National|        U.S.|         News|\n",
      "|   1039|      2005-09-29|      1133|            18|            1|                9|    true|       1.0|            A| National|        U.S.|News Analysis|\n",
      "|   1040|      2015-04-11|      1044|             8|            1|               10|    true|       9.0|            A| National|        U.S.|         News|\n",
      "+-------+----------------+----------+--------------+-------------+-----------------+--------+----------+-------------+---------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#clean the print_page column in facts df\n",
    "#Some print pages aren't numeric: 25A - A is already in print_section column\n",
    "#Create a list of unique page numbers \n",
    "page_rdd = facts.select(['print_page']).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "#include pages that aren't None\n",
    "pages = [p for p in set(page_rdd) if p != None]\n",
    "\n",
    "pages_cleaned = {}\n",
    "for page in pages:\n",
    "    if page != None:\n",
    "        try:\n",
    "            p = float(page)\n",
    "            p_dict = {page:p}\n",
    "            pages_cleaned.update(p_dict)\n",
    "        except:\n",
    "            p = page[:-1]\n",
    "            p = float(p)\n",
    "            p_dict = {page:p}\n",
    "            pages_cleaned.update(p_dict)\n",
    "    else:\n",
    "        pass\n",
    "#create mapping\n",
    "mapping = f.create_map([f.lit(x) for x in chain(*pages_cleaned.items())])\n",
    "\n",
    "#update the print_page column with mapping\n",
    "facts = facts.withColumn('print_page', mapping[facts['print_page']])\n",
    "facts.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----+-------+--------------------+-----+---------+------------+------+---------+\n",
      "|table_id|fact_id|rank|   name|               value|major|last_name|first_middle|middle|qualifier|\n",
      "+--------+-------+----+-------+--------------------+-----+---------+------------+------+---------+\n",
      "|1006.002|   1006|   8|persons|      Bush, George W|    N|     BUSH|        NULL|  NULL|     NULL|\n",
      "|1001.002|   1001|   7|persons|Kelley, Devin P (...|    N|   KELLEY|        NULL|  NULL|     NULL|\n",
      "|1005.002|   1005|   4|persons|            Gore, Al|    N|     GORE|        NULL|  NULL|     NULL|\n",
      "|1013.001|   1013|   1|persons|      Keller, Sharon|    N|   KELLER|        NULL|  NULL|     NULL|\n",
      "|1014.001|   1014|   4|persons|        Davis, Wendy|    N|    DAVIS|        NULL|  NULL|     NULL|\n",
      "|1002.001|   1002|   3|persons| Gonzales, Alberto R|    N| GONZALES|        NULL|  NULL|     NULL|\n",
      "|1007.002|   1007|   5|persons| Gamble, Maya Guerra|    N|   GAMBLE|        NULL|  NULL|     NULL|\n",
      "|1012.001|   1012|   6|persons|  Kavanaugh, Brett M|    N|KAVANAUGH|        NULL|  NULL|     NULL|\n",
      "|1003.001|   1003|   3|persons|       Obama, Barack|    N|    OBAMA|        NULL|  NULL|     NULL|\n",
      "|1010.001|   1010|   3|persons|         Perry, Rick|    N|    PERRY|        NULL|  NULL|     NULL|\n",
      "|1001.001|   1001|   5|persons|     Trump, Donald J|    N|    TRUMP|        NULL|  NULL|     NULL|\n",
      "|1000.001|   1000|   2|persons|    ROACH, RICHARD J|    N|    ROACH|        NULL|  NULL|     NULL|\n",
      "|1003.002|   1003|   9|persons|     Hanen, Andrew S|    N|    HANEN|        NULL|  NULL|     NULL|\n",
      "|1017.002|   1017|  12|persons|Christie, Christo...|    N| CHRISTIE|        NULL|  NULL|     NULL|\n",
      "|1002.002|   1002|   4|persons|          DeLay, Tom|    N|    DELAY|        NULL|  NULL|     NULL|\n",
      "|1007.001|   1007|   4|persons|Abbott, Gregory W...|    N|   ABBOTT|        NULL|  NULL|     NULL|\n",
      "|1002.003|   1002|   5|persons|      Bush, George W|    N|     BUSH|        NULL|  NULL|     NULL|\n",
      "|1005.001|   1005|   3|persons|      Bush, George W|    N|     BUSH|        NULL|  NULL|     NULL|\n",
      "|1015.001|   1015|   2|persons|  McRaven, William H|    N|  MCRAVEN|        NULL|  NULL|     NULL|\n",
      "|1006.001|   1006|   7|persons|     Duncan, David B|    N|   DUNCAN|        NULL|  NULL|     NULL|\n",
      "+--------+-------+----+-------+--------------------+-----+---------+------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IN the people df create a standardized first name, middle name, and last name columns\n",
    "\n",
    "# Get the last name (left of comma) and upper case it\n",
    "people = people.withColumn('last_name', f.upper(f.substring_index('value', ',', 1)))\n",
    "# Get the first and middle name (right of comma) and upper case it\n",
    "people = people.withColumn('first_middle', f.upper(f.substring_index('value', ',', -1)))\n",
    "# Replace special characters in first and middle names with empty string,\n",
    "# But keep inner white space to separate middle name from first\n",
    "people = people.withColumn('first_middle', f.trim(f.regexp_replace('first_middle', '[^a-zA-Z ]', '')))\n",
    "# Identify name qualifiers - SR, JR, II, III, IV, V\n",
    "# EX - Dale Earnhardt vs Dale Earnhardt Jr\n",
    "qualifier_list = ['JR', 'SR', 'II', 'III', 'IV', 'V']\n",
    "qualifiers = people.where(f.substring_index('first_middle', ' ', -1).isin(qualifier_list)).select(['table_id', 'first_middle'])\n",
    "\n",
    "#Map replacement string for names with ending qualifiers\n",
    "name_replace = qualifiers.select('first_middle')\n",
    "name_list = name_replace.select(['first_middle']).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "name_dict = {name:name[0:name.rindex(' ')] for name in name_list}\n",
    "#create mapping\n",
    "name_map = f.create_map([f.lit(x) for x in chain(*name_dict.items())])\n",
    "people = people.withColumn('first_middle', name_map[people['first_middle']])\n",
    "\n",
    "#Get middle name / initial\n",
    "people = people.withColumn('middle', \n",
    "    f.when(f.size(f.split(people['first_middle'], ' ', -1)) == 1, None).otherwise(f.substring_index(people['first_middle'], ' ', -1)))\n",
    "\n",
    "#Merge qualifiers back in as a separate column\n",
    "qualifiers = qualifiers.withColumn('qualifier', f.substring_index('first_middle', ' ', -1)).select('table_id', 'qualifier')\n",
    "people = people.join(qualifiers, ['table_id'], how = 'left')\n",
    "\n",
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----+-------+--------------------+-----+---------+------------+------+---------+\n",
      "|table_id|fact_id|rank|   name|               value|major|last_name|first_middle|middle|qualifier|\n",
      "+--------+-------+----+-------+--------------------+-----+---------+------------+------+---------+\n",
      "|1006.002|   1006|   8|persons|      Bush, George W|    N|     BUSH|        NULL|  NULL|     NULL|\n",
      "|1001.002|   1001|   7|persons|Kelley, Devin P (...|    N|   KELLEY|        NULL|  NULL|     NULL|\n",
      "|1005.002|   1005|   4|persons|            Gore, Al|    N|     GORE|        NULL|  NULL|     NULL|\n",
      "|1013.001|   1013|   1|persons|      Keller, Sharon|    N|   KELLER|        NULL|  NULL|     NULL|\n",
      "|1014.001|   1014|   4|persons|        Davis, Wendy|    N|    DAVIS|        NULL|  NULL|     NULL|\n",
      "|1002.001|   1002|   3|persons| Gonzales, Alberto R|    N| GONZALES|        NULL|  NULL|     NULL|\n",
      "|1007.002|   1007|   5|persons| Gamble, Maya Guerra|    N|   GAMBLE|        NULL|  NULL|     NULL|\n",
      "|1012.001|   1012|   6|persons|  Kavanaugh, Brett M|    N|KAVANAUGH|        NULL|  NULL|     NULL|\n",
      "|1003.001|   1003|   3|persons|       Obama, Barack|    N|    OBAMA|        NULL|  NULL|     NULL|\n",
      "|1010.001|   1010|   3|persons|         Perry, Rick|    N|    PERRY|        NULL|  NULL|     NULL|\n",
      "|1001.001|   1001|   5|persons|     Trump, Donald J|    N|    TRUMP|        NULL|  NULL|     NULL|\n",
      "|1000.001|   1000|   2|persons|    ROACH, RICHARD J|    N|    ROACH|        NULL|  NULL|     NULL|\n",
      "|1003.002|   1003|   9|persons|     Hanen, Andrew S|    N|    HANEN|        NULL|  NULL|     NULL|\n",
      "|1017.002|   1017|  12|persons|Christie, Christo...|    N| CHRISTIE|        NULL|  NULL|     NULL|\n",
      "|1002.002|   1002|   4|persons|          DeLay, Tom|    N|    DELAY|        NULL|  NULL|     NULL|\n",
      "|1007.001|   1007|   4|persons|Abbott, Gregory W...|    N|   ABBOTT|        NULL|  NULL|     NULL|\n",
      "|1002.003|   1002|   5|persons|      Bush, George W|    N|     BUSH|        NULL|  NULL|     NULL|\n",
      "|1005.001|   1005|   3|persons|      Bush, George W|    N|     BUSH|        NULL|  NULL|     NULL|\n",
      "|1015.001|   1015|   2|persons|  McRaven, William H|    N|  MCRAVEN|        NULL|  NULL|     NULL|\n",
      "|1006.001|   1006|   7|persons|     Duncan, David B|    N|   DUNCAN|        NULL|  NULL|     NULL|\n",
      "+--------+-------+----+-------+--------------------+-----+---------+------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JOSEPH R', 'WILLIAM R']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name[0:name.rindex(' ')] for name in name_list]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
