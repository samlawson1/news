{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nyt_article_search import JSONParse\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load JSONs and transform in to lists of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extend each list as we parse the jsons\n",
    "# The get_article_authors and  search_article_keywords methods\n",
    "# return a list of tuples so this will create one master list of all tuples\n",
    "def extend_list(list_name, function_result):\n",
    "    if function_result != None:\n",
    "        list_name.extend(function_result)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists that the data from each json response will be added to\n",
    "article_ids = []\n",
    "fact_data = []\n",
    "author_data = []\n",
    "subject_data = []\n",
    "people_data = []\n",
    "org_data = []\n",
    "loc_data = []\n",
    "# Big text will be a dictionary where we'll add headline, lead paragraph,\n",
    "# abstract and web url\n",
    "big_text = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'DATA'\n",
    "state = 'TEXAS'\n",
    "# states = [f for f in os.listdir(data_folder) if f != '_Archive']\n",
    "state_folder = os.path.join(data_folder, state)\n",
    "json_files = os.listdir(state_folder)\n",
    "for file in json_files:\n",
    "    filepath = os.path.join(state_folder, file)\n",
    "    json_file = open(filepath, 'r')\n",
    "    data = json.load(json_file)\n",
    "    responses = data['response']['docs']\n",
    "    # Parse each json response in the json file\n",
    "    for respsonse in responses:\n",
    "        j = JSONParse(respsonse)\n",
    "        article_ids.append(j.article_id)\n",
    "        fact_data.append(j.get_article_facts())\n",
    "        extend_list(author_data, j.get_article_authors())\n",
    "        extend_list(subject_data, j.search_article_keywords('subject'))\n",
    "        extend_list(people_data, j.search_article_keywords('persons'))\n",
    "        extend_list(org_data, j.search_article_keywords('organizations'))\n",
    "        extend_list(loc_data, j.search_article_keywords('glocations'))\n",
    "        id_text_dict = {\n",
    "            j.article_id:{\n",
    "                            'headline':j.get_text('headline')[1],\n",
    "                            'abstract':j.get_text('abstract')[1],\n",
    "                            'lead_paragraph':j.get_text('lead_paragraph')[1],\n",
    "                            'web_url':j.get_text('web_url')[1]\n",
    "                        }\n",
    "            }\n",
    "        big_text.update(id_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName('NYT_JSON_ETL').\\\n",
    "        master('local[1]').\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each result get the column headers for the resulting\n",
    "#dataframe\n",
    "def get_table_headers(table):\n",
    "\n",
    "    table_col_dict = {\n",
    "        'facts':['article_id', 'publication_date',\n",
    "                            'word_count', 'total_keywords',\n",
    "                            'total_authors', 'words_in_headline',\n",
    "                            'in_print', 'print_page', 'print_section',\n",
    "                            'news_desk', 'section_name', 'article_type'],\n",
    "        'authors':['article_id', 'rank', 'role',\n",
    "                            'firstname', 'middlename', 'lastname', 'qualifier'],\n",
    "        'subjects':['article_id', 'rank', 'name', 'value', 'major'],\n",
    "        'text':['article_id', 'text']\n",
    "    }\n",
    "\n",
    "    headers = table_col_dict.get(table)\n",
    "    return(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create spark dfs\n",
    "facts = spark.createDataFrame(fact_data, schema=get_table_headers('facts'))\n",
    "authors = spark.createDataFrame(author_data, schema=get_table_headers('authors'))\n",
    "subjects = spark.createDataFrame(subject_data, schema=get_table_headers('subjects'))\n",
    "people = spark.createDataFrame(people_data, schema=get_table_headers('subjects'))\n",
    "organizations = spark.createDataFrame(org_data, schema=get_table_headers('subjects'))\n",
    "locations = spark.createDataFrame(loc_data, schema=get_table_headers('subjects'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Primary Keys in tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to create an interger primary key for the article_ids\n",
    "ids = [(e + 1000, i) for e, i in enumerate(article_ids)]\n",
    "# Create spark df out of id list\n",
    "# Integer Primary Key is fact_id\n",
    "id_schema = StructType([\n",
    "    StructField('fact_id', IntegerType(), False),\n",
    "    StructField('article_id', StringType(), False)\n",
    "    ])\n",
    "\n",
    "### THIS SHOULD NOT CHANGE ANY FURTHER\n",
    "id_df = spark.createDataFrame(ids, schema=id_schema)\n",
    "\n",
    "\n",
    "\n",
    "#Merge the id_df dataframe into the existing frames\n",
    "# To put fact_id in all the other tables\n",
    "facts = id_df.join(facts, ['article_id'], how = 'inner').drop('article_id')\n",
    "authors = id_df.join(authors, ['article_id'], how = 'inner').drop('article_id')\n",
    "subjects = id_df.join(subjects, ['article_id'], how = 'inner').drop('article_id')\n",
    "people = id_df.join(people, ['article_id'], how = 'inner').drop('article_id')\n",
    "organizations = id_df.join(organizations, ['article_id'], how = 'inner').drop('article_id')\n",
    "locations = id_df.join(locations, ['article_id'], how = 'inner').drop('article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union the subjects, organizations, and locations dfs togehter\n",
    "places_and_things = subjects.union(organizations).union(locations).orderBy(['fact_id', 'rank'])\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create window function to partition by id and order by rank\n",
    "window = Window().partitionBy('fact_id').orderBy('rank')\n",
    "# Create primary key for places_and_things, authors, and people dataframes\n",
    "def create_primary_key(df, key_name, article_id, w):\n",
    "    #Create window function to partition by id and order by rank\n",
    "    window = w\n",
    "    #Row number\n",
    "    df = df.withColumn(key_name, f.row_number().over(window))\n",
    "    #Divide each row number by 1000 to get a decimal representation\n",
    "    df = df.withColumn(key_name, f.col(key_name) / 1000)\n",
    "    #Add the decimal to the id column (primary key for fact dataframe) to create a logical\n",
    "    #Key representation\n",
    "    df = df.withColumn(key_name, f.col(article_id) + f.col(key_name))\n",
    "    return(df)\n",
    "\n",
    "places_and_things = create_primary_key(places_and_things, 'table_id', 'fact_id', window)\n",
    "authors = create_primary_key(authors, 'table_id', 'fact_id', window)\n",
    "people = create_primary_key(people, 'table_id', 'fact_id', window)\n",
    "places_and_things.show(2)\n",
    "authors.show(2)\n",
    "people.show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Text standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_folder = os.path.join('DATA', 'DIM_TABLES')\n",
    "dim_news_desk = os.path.join(dim_folder, 'news_desks.csv')\n",
    "dim_types = os.path.join(dim_folder, 'article_types.csv')\n",
    "dim_sections = os.path.join(dim_folder, 'section_names.csv')\n",
    "\n",
    "news_desk_df = spark.read.option('header', True).csv(dim_news_desk)\n",
    "material_df = spark.read.option('header', True).csv(dim_types)\n",
    "section_df = spark.read.option('header', True).csv(dim_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(clean_df, clean_col, dim_df):\n",
    "    #clean_df = the dataframe to be cleaned\n",
    "    #clean_col = the column to be cleaned\n",
    "    #dim_df = the dimensional table to clean with\n",
    "\n",
    "    #replace any 'None' strings with None type\n",
    "    clean_df = clean_df.withColumn(clean_col, f.when(clean_df[clean_col] == 'None', None).otherwise(clean_df[clean_col]))\n",
    "    #Get distinct values where not null\n",
    "    distincts = clean_df.where(clean_df[clean_col].isNotNull()).select(clean_df[clean_col]).distinct()\n",
    "    #Get values that aren't in the dimensional tables\n",
    "    distincts = distincts.join(dim_df, [clean_col], how = 'left')\n",
    "    #get the id column from the dimensional table\n",
    "    id_col = f'{clean_col}_id'\n",
    "    dirty_text = distincts.where(distincts[id_col].isNull()).select(distincts[clean_col].alias('dirty_text'))\n",
    "    #Cross join and calculate levenshtein distance\n",
    "    dirty_text = dirty_text.crossJoin(dim_df).select(['dirty_text', clean_col])\n",
    "    dirty_text = dirty_text.withColumn('levenshtein', f.levenshtein('dirty_text', clean_col))\n",
    "\n",
    "    #Get min levenshtein distance for each dirty text\n",
    "    #Min = best match\n",
    "    min_lev = dirty_text.groupBy('dirty_text').agg(f.min('levenshtein').alias('levenshtein'))\n",
    "    dirty_text = dirty_text.join(min_lev, ['dirty_text' , 'levenshtein'])\n",
    "\n",
    "    #In case there are no good matches use row number and grab first one\n",
    "    window = Window().partitionBy('dirty_text').orderBy('dirty_text')\n",
    "    dirty_text = dirty_text.withColumn('row_num', f.row_number().over(window))\n",
    "    dirty_text = dirty_text.where(dirty_text['row_num']== 1).select(['dirty_text', clean_col])\n",
    "    text_mapping = {row['dirty_text']:row[clean_col] for row in dirty_text.rdd.collect()}\n",
    "    clean_df = clean_df.replace(text_mapping, subset = [clean_col])\n",
    "    return(clean_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = clean_text(facts, 'article_type', material_df)\n",
    "facts = clean_text(facts, 'news_desk', news_desk_df)\n",
    "facts = clean_text(facts, 'section_name', section_df)\n",
    "facts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_rdd = facts.select(['print_page']).rdd.flatMap(lambda x: x).collect()\n",
    "#unique pages in df\n",
    "pages = list(set(page_rdd))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_cleaned = {}\n",
    "for page in pages:\n",
    "    if page != None:\n",
    "        try:\n",
    "            p = float(page)\n",
    "            p_dict = {page:p}\n",
    "            pages_cleaned.update(p_dict)\n",
    "        except:\n",
    "            p = page[:-1]\n",
    "            p = float(p)\n",
    "            p_dict = {page:p}\n",
    "            pages_cleaned.update(p_dict)\n",
    "    else:\n",
    "        # p = None\n",
    "        # p_dict = {page:p}\n",
    "        # pages_cleaned.update(p_dict)\n",
    "        pass\n",
    "\n",
    "mapping = f.create_map([f.lit(x) for x in chain(*pages_cleaned.items())])\n",
    "\n",
    "# facts.select(mapping[facts['print_page']].alias('page_clean')).show()\n",
    "facts = facts.withColumn('print_page', mapping[facts['print_page']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
