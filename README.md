# News - The New York Times##  **Purpose**The _main_ purpose of this repository is to showcase my use of more advanced analytical tools like Apache Spark, Apache Iceberg, and TensorFlow to produce end-to-end data engineering and machine learning pipelines. [The New York Times provides a host of API's](https://developer.nytimes.com/apis) that make this possible. For this project, I've chosen to focus on their [article search api](https://developer.nytimes.com/docs/articlesearch-product/1/overview) to perform a sentiment analysis on article headlines, abstracts, and lead paragraphs when the subject of the article focuses on one of the 50 states in the United States, and compare how or if the sentiment of how those text fields are written differ among various states.##  **Overview**### Part 1 - Data CollectionTo collect the data, a basic python script is all that is needed to call the API and store the responses. Certain filters are necessary to meet the purpose of the project, which are:- Filtering on The New York Times as the source- Only including articles since January 1st, 2000- Filtering on a specific state that is part of the article subjects- Calling the API 10 times per state. The API is paginated, with 10 results per response. We can expect 100 articles for each state.Given the above criteria, the following function was created.`def get_paginated_responses(state, i):    #get the api key needed to request data    api_key = os.environ['API_KEY']    #Make state folders as you iterate in current working directory    out_folder = 'DATA'    state_path = os.path.join(out_folder, state)    if not os.path.exists(state_path):        os.mkdir(state_path)    #query filters    source = r'source:("The New York Times")'    state_query = f'glocations:("{state}")'    #file name. i = the paginated. There will be 10 total responses in each paginated response    file = f'{state}_page_{i}.json'    #define the output    out_path = os.path.join(state_path, file)    #Search articles about each state written by The New York Times since 2000    request_url = f'https://api.nytimes.com/svc/search/v2/articlesearch.json?fq={state_query} AND {source}\        &begin_date=20000101&page={i}&api-key={api_key}'    r = requests.get(request_url)    api_response = json.loads(r.content)    #save the json file    with open(out_path, 'w') as f:        json.dump(api_response, f)`