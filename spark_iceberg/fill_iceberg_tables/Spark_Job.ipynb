{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom\n",
    "from nyt_article_search import JSONParse\n",
    "from spark_job_functions import *\n",
    "#Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "#Python standard library\n",
    "from itertools import chain\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load JSONs and transform in to lists of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists that the data from each json response will be added to\n",
    "article_ids = []\n",
    "fact_data = []\n",
    "author_data = []\n",
    "subject_data = []\n",
    "people_data = []\n",
    "org_data = []\n",
    "loc_data = []\n",
    "# Big text will be a dictionary where we'll add headline, lead paragraph,\n",
    "# abstract and web url\n",
    "big_text = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALABAMA', 'ALASKA', 'ARIZONA', 'ARKANSAS', 'CALIFORNIA', 'COLORADO', 'CONNECTICUT', 'DELAWARE', 'FLORIDA', 'GEORGIA', 'HAWAII', 'IDAHO', 'ILLINOIS', 'INDIANA', 'IOWA', 'KANSAS', 'KENTUCKY', 'LOUISIANA', 'MAINE', 'MARYLAND', 'MASSACHUSETTS', 'MICHIGAN', 'MINNESOTA', 'MISSISSIPPI', 'MISSOURI', 'MONTANA', 'NEBRASKA', 'NEVADA', 'NEW HAMPSHIRE', 'NEW JERSEY', 'NEW MEXICO', 'NEW YORK', 'NORTH CAROLINA', 'NORTH DAKOTA', 'OHIO', 'OKLAHOMA', 'OREGON', 'PENNSYLVANIA', 'RHODE ISLAND', 'SOUTH CAROLINA', 'SOUTH DAKOTA', 'TENNESSEE', 'TEXAS', 'UTAH', 'VERMONT', 'VIRGINIA', 'WASHINGTON', 'WEST VIRGINIA', 'WISCONSIN', 'WYOMING']\n"
     ]
    }
   ],
   "source": [
    "#Data folder where JSONs are located from api_call\n",
    "# - Should be 1 folder for every state\n",
    "data_folder = r'../../api_call/DATA'\n",
    "print(os.listdir(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "states = [f for f in os.listdir(data_folder)]\n",
    "for state in states:\n",
    "    state_folder = os.path.join(data_folder, state)\n",
    "    json_files = os.listdir(state_folder)\n",
    "    for file in json_files:\n",
    "        filepath = os.path.join(state_folder, file)\n",
    "        json_file = open(filepath, 'r')\n",
    "        data = json.load(json_file)\n",
    "        responses = data['response']['docs']\n",
    "        # Parse each json response in the json file\n",
    "        for respsonse in responses:\n",
    "            j = JSONParse(respsonse)\n",
    "            #Some JSON files share the same articles\n",
    "            #Only append if the article_id is not currently in the list\n",
    "            if j.article_id not in article_ids:\n",
    "                article_ids.append(j.article_id)\n",
    "                fact_data.append(j.get_article_facts())\n",
    "                extend_list(author_data, j.get_article_authors())\n",
    "                extend_list(subject_data, j.search_article_keywords('subject'))\n",
    "                extend_list(people_data, j.search_article_keywords('persons'))\n",
    "                extend_list(org_data, j.search_article_keywords('organizations'))\n",
    "                extend_list(loc_data, j.search_article_keywords('glocations'))\n",
    "                id_text_dict = {\n",
    "                    j.article_id:{\n",
    "                                    'headline':j.get_text('headline')[1],\n",
    "                                    'abstract':j.get_text('abstract')[1],\n",
    "                                    'lead_paragraph':j.get_text('lead_paragraph')[1],\n",
    "                                    'web_url':j.get_text('web_url')[1]\n",
    "                                }\n",
    "                    }\n",
    "                big_text.update(id_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iceberg_warehouse = os.environ.get('ICEBERG_WAREHOUSE')\n",
    "spark = SparkSession.builder.config(conf=spark_config(iceberg_warehouse)).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create spark dfs\n",
    "facts = spark.createDataFrame(fact_data, schema=get_table_headers('facts'))\n",
    "authors = spark.createDataFrame(author_data, schema=get_table_headers('authors'))\n",
    "subjects = spark.createDataFrame(subject_data, schema=get_table_headers('subjects'))\n",
    "people = spark.createDataFrame(people_data, schema=get_table_headers('subjects'))\n",
    "organizations = spark.createDataFrame(org_data, schema=get_table_headers('subjects'))\n",
    "locations = spark.createDataFrame(loc_data, schema=get_table_headers('subjects'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Primary Keys in tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to create an interger primary key for the article_ids\n",
    "ids = [(e + 1000, i) for e, i in enumerate(article_ids)]\n",
    "# Create spark df out of id list\n",
    "# Integer Primary Key is fact_id\n",
    "id_schema = StructType([\n",
    "    StructField('fact_id', IntegerType(), False),\n",
    "    StructField('article_id', StringType(), False)\n",
    "    ])\n",
    "\n",
    "### THIS SHOULD NOT CHANGE ANY FURTHER\n",
    "id_df = spark.createDataFrame(ids, schema=id_schema)\n",
    "\n",
    "\n",
    "\n",
    "#Merge the id_df dataframe into the existing frames\n",
    "# To put fact_id in all the other tables\n",
    "facts = id_df.join(facts, ['article_id'], how = 'inner').drop('article_id')\n",
    "authors = id_df.join(authors, ['article_id'], how = 'inner').drop('article_id')\n",
    "subjects = id_df.join(subjects, ['article_id'], how = 'inner').drop('article_id')\n",
    "people = id_df.join(people, ['article_id'], how = 'inner').drop('article_id')\n",
    "organizations = id_df.join(organizations, ['article_id'], how = 'inner').drop('article_id')\n",
    "locations = id_df.join(locations, ['article_id'], how = 'inner').drop('article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union the subjects, organizations, and locations dfs togehter\n",
    "places_and_things = subjects.union(organizations).union(locations).orderBy(['fact_id', 'rank'])\n",
    "places_and_things = create_primary_key(places_and_things, 'table_id', 'fact_id', 'rank')\n",
    "authors = create_primary_key(authors, 'table_id', 'fact_id', 'rank')\n",
    "people = create_primary_key(people, 'table_id', 'fact_id', 'rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Text standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in dimensional tables\n",
    "\n",
    "news_desk_df = spark.table('nyt.db.dim_news_desks')\n",
    "material_df = spark.table('nyt.db.dim_article_types')\n",
    "section_df = spark.table('nyt.db.dim_section_names')\n",
    "subject_id_df = spark.table('nyt.db.dim_subject_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize any text in the article_type, news_desk, and section_name\n",
    "#columns that aren't in the dimensinal tables\n",
    "#EX: National Desk vs National - should be National\n",
    "facts = standardize_text(facts, 'article_type', material_df)\n",
    "facts = standardize_text(facts, 'news_desk', news_desk_df)\n",
    "facts = standardize_text(facts, 'section_name', section_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the print_page column in facts df\n",
    "#Some print pages aren't numeric: 25A - A is already in print_section column\n",
    "#Create a list of unique page numbers \n",
    "page_rdd = facts.select(['print_page']).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "#include pages that aren't None\n",
    "pages = [p for p in set(page_rdd) if p != None]\n",
    "\n",
    "pages_cleaned = {}\n",
    "for page in pages:\n",
    "    if page != None:\n",
    "        try:\n",
    "            p = float(page)\n",
    "            p_dict = {page:p}\n",
    "            pages_cleaned.update(p_dict)\n",
    "        except:\n",
    "            if page == '':\n",
    "                p = None\n",
    "                p_dict = {page:p}\n",
    "                pages_cleaned.update(p_dict)\n",
    "            else:\n",
    "                p = page[:-1]\n",
    "                p = float(p)\n",
    "                p_dict = {page:p}\n",
    "                pages_cleaned.update(p_dict)\n",
    "    else:\n",
    "        pass\n",
    "#create mapping\n",
    "mapping = f.create_map([f.lit(x) for x in chain(*pages_cleaned.items())])\n",
    "\n",
    "#update the print_page column with mapping\n",
    "facts = facts.withColumn('print_page', mapping[facts['print_page']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN the people df create a standardized first name, middle name, and last name columns\n",
    "\n",
    "# Get the last name (left of comma) and upper case it\n",
    "people = people.withColumn('last_name', f.upper(f.substring_index('value', ',', 1)))\n",
    "# Get the first and middle name (right of comma) and upper case it\n",
    "people = people.withColumn('first_middle', f.upper(f.substring_index('value', ',', -1)))\n",
    "# Replace special characters in first and middle names with empty string,\n",
    "# But keep inner white space to separate middle name from first\n",
    "people = people.withColumn('first_middle', f.trim(f.regexp_replace('first_middle', '[^a-zA-Z ]', '')))\n",
    "# Identify name qualifiers - SR, JR, II, III, IV, V\n",
    "# EX - Dale Earnhardt vs Dale Earnhardt Jr\n",
    "qualifier_list = ['JR', 'SR', 'II', 'III', 'IV', 'V']\n",
    "qualifiers = people.where(f.substring_index('first_middle', ' ', -1).isin(qualifier_list)).select(['table_id', 'first_middle'])\n",
    "\n",
    "#Map replacement string for names with ending qualifiers\n",
    "name_replace = qualifiers.select('first_middle')\n",
    "name_list = name_replace.select(['first_middle']).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "name_dict = {name:name[0:name.rindex(' ')] for name in name_list}\n",
    "\n",
    "#update name_dict to include rest of names so mapping is correct\n",
    "non_qualifiers = people.where(~f.substring_index('first_middle', ' ', -1).isin(qualifier_list)).select(['table_id', 'first_middle'])\n",
    "non_qual_list = non_qualifiers.select(['first_middle']).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "for n in non_qual_list:\n",
    "    name_dict.update({n:n})\n",
    "\n",
    "\n",
    "#create mapping\n",
    "name_map = f.create_map([f.lit(x) for x in chain(*name_dict.items())])\n",
    "people = people.withColumn('first_middle', name_map[people['first_middle']])\n",
    "\n",
    "#Get middle name / initial\n",
    "people = people.withColumn('middle', \n",
    "    f.when(f.size(f.split(people['first_middle'], ' ', -1)) == 1, None).otherwise(f.substring_index(people['first_middle'], ' ', -1)))\n",
    "\n",
    "#Get first name\n",
    "people = people.withColumn('first_name', f.substring_index(people['first_middle'], ' ', 1))\n",
    "\n",
    "#Merge qualifiers back in as a separate column\n",
    "qualifiers = qualifiers.withColumn('qualifier', f.substring_index('first_middle', ' ', -1)).select('table_id', 'qualifier')\n",
    "people = people.join(qualifiers, ['table_id'], how = 'left').drop('value').drop('first_middle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors - rename columns to match people df\n",
    "rename_authors_cols = {\n",
    "    'lastname':'last_name',\n",
    "    'firstname':'first_name',\n",
    "    'middlename':'middle', \n",
    "    'rank':'role_rank',\n",
    "    'role':'author_role'\n",
    "}\n",
    "authors = authors.withColumnsRenamed(rename_authors_cols)\n",
    "\n",
    "#uppercase names\n",
    "authors = authors.withColumn('last_name', f.upper('last_name'))\n",
    "authors = authors.withColumn('first_name', f.upper('first_name'))\n",
    "authors = authors.withColumn('middle', f.upper('middle'))\n",
    "\n",
    "\n",
    "#people and plances_and_things - update columns\n",
    "peole_col_renamed = {\n",
    "    'rank':'subject_rank',\n",
    "    'major':'major_subject'\n",
    "}\n",
    "people = people.withColumnsRenamed(peole_col_renamed)\n",
    "\n",
    "places_and_things_col_renamed = {\n",
    "    'rank':'subject_rank',\n",
    "    'major':'major_subject',\n",
    "    'value':'subject'\n",
    "}\n",
    "places_and_things = places_and_things.withColumn('value', f.upper('value'))\n",
    "places_and_things = places_and_things.withColumnsRenamed(places_and_things_col_renamed)\n",
    "\n",
    "#Make major_role column true or false\n",
    "people = people.withColumn('major_subject', f.when(people['major_subject'] == 'Y', True).otherwise(False))\n",
    "places_and_things = places_and_things.withColumn('major_subject', f.when(places_and_things['major_subject'] == 'Y', True).otherwise(False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Use Dimensional Id's instead of text\n",
    "\n",
    "- Dimensional Tables:\n",
    "    - article_type_ids: material_df\n",
    "    - news_desk_ids: news_desk_df\n",
    "    - section_ids: section_df\n",
    "    - subjects_ids: create df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Sure Id's are ints\n",
    "material_df = material_df.withColumn('article_type_id', material_df['article_type_id'].cast(IntegerType()))\n",
    "news_desk_df = news_desk_df.withColumn('news_desk_id', news_desk_df['news_desk_id'].cast(IntegerType()))\n",
    "section_df = section_df.withColumn('section_name_id', section_df['section_name_id'].cast(IntegerType()))\n",
    "subject_id_df = subject_id_df.withColumn('subject_id', subject_id_df['subject_id'].cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "facts = facts.join(material_df, ['article_type'], how = 'left')\\\n",
    "        .join(news_desk_df, ['news_desk'], how = 'left')\\\n",
    "        .join(section_df, ['section_name'], how = 'left')\\\n",
    "        .drop(*('section_name', 'news_desk', 'article_type'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join subject id to places_and_things df\n",
    "subject_join = subject_id_df.withColumnRenamed('subject_name', 'name')\n",
    "places_and_things = places_and_things.join(subject_join, ['name'], how = 'left').drop('name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = facts.select(final_col_order('facts'))\n",
    "authors = authors.select(final_col_order('authors'))\n",
    "#add subject id to people\n",
    "people = people.withColumn('subject_id', f.lit(1))\n",
    "people = people.select(final_col_order('subject_people'))\n",
    "places_and_things = places_and_things.select(final_col_order('subject_others'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_check_dict = {}\n",
    "quality_checker = [\n",
    "            (id_df, 'article_id', 'article_ids'),\n",
    "            (facts, 'fact_id', 'facts'), \n",
    "            (authors, 'table_id', 'authors'),\n",
    "            (people, 'table_id', 'subject_people'),\n",
    "            (places_and_things, 'table_id', 'subject_others')\n",
    "            ]\n",
    "\n",
    "for table in quality_checker:\n",
    "    dup_check = duplicate_check(table[0], table[1], table[2])\n",
    "    quality_check_dict.update(dup_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 64390)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"d:\\Projects\\news\\lib\\site-packages\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"d:\\Projects\\news\\lib\\site-packages\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"d:\\Projects\\news\\lib\\site-packages\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"d:\\Projects\\news\\lib\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Projects\\news\\lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Projects\\news\\lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"d:\\Projects\\news\\lib\\site-packages\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[19], line 20\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#Partitions\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mfacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnyt.db.facts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionedBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpublication_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m places_and_things\u001b[38;5;241m.\u001b[39mwriteTo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnyt.db.subject_others\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mpartitionedBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mappend()\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\pyspark\\sql\\readwriter.py:2107\u001b[0m, in \u001b[0;36mDataFrameWriterV2.append\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;124;03mAppend the contents of the data frame to the output table.\u001b[39;00m\n\u001b[0;32m   2106\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2107\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(10061, 'No connection could be made because the target machine actively refused it', None, 10061, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2179\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2176\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m   2177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[0;32m   2182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\ipykernel\\zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[1;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[0;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\py4j\\protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[1;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "# #If the quality check passes, write the tables\n",
    "if all(quality_check_dict.values()) == True:\n",
    "    #Validate Schemas\n",
    "    facts = spark.createDataFrame(facts.rdd.collect(), schema=StructType(schema_validation('facts')))\n",
    "    authors = spark.createDataFrame(authors.rdd.collect(), schema=StructType(schema_validation('authors')))\n",
    "    people = spark.createDataFrame(people.rdd.collect(), schema=StructType(schema_validation('subject_people')))\n",
    "    places_and_things = spark.createDataFrame(places_and_things.rdd.collect(), schema=StructType(schema_validation('subject_others')))\n",
    "    id_df = spark.createDataFrame(id_df.rdd.collect(), schema=StructType(schema_validation('article_ids')))\n",
    "\n",
    "    #Sort partitioned tables\n",
    "    facts = facts.sort(facts['publication_date'])\n",
    "    places_and_things = places_and_things.sort(places_and_things['subject_id'])\n",
    "\n",
    "    #Write to iceberg tables\n",
    "    #No partitions\n",
    "    authors.writeTo('nyt.db.authors').append()\n",
    "    people.writeTo('nyt.db.subject_people').append()\n",
    "    id_df.writeTo('nyt.db.article_ids').append()\n",
    "    #Partitions\n",
    "    facts.writeTo('nyt.db.facts').partitionedBy('publication_date').append()\n",
    "    places_and_things.writeTo('nyt.db.subject_others').partitionedBy('subject_id').append()\n",
    "else:\n",
    "    table_failures = [k for k, v in quality_check_dict.items() if v == False]\n",
    "    tables_joined = ', '.join(table_failures)\n",
    "    message = f'The following tables failed the duplicate primary key quality check: {tables_joined}'\n",
    "    print(message)\n",
    "    raise Exception(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write text files for sentiment analysis\n",
    "\n",
    "#Create directory if it doesn't exist\n",
    "data_folder = '..\\DATA'\n",
    "if not os.path.exists(data_folder):\n",
    "    os.mkdir(data_folder)\n",
    "\n",
    "analysis_folder = os.path.join(data_folder, 'SENTIMENT_ANALYSIS_FILES')\n",
    "if not os.path.exists(analysis_folder):\n",
    "    os.mkdir(analysis_folder)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to write text files for sentiment analysis\n",
    "def write_text_file(text_type):\n",
    "    text = v[text_type]\n",
    "    if text != None:\n",
    "        text_file = os.path.join(analysis_folder, f'{i}-{text_type}.txt')\n",
    "        text_writer = open(text_file, 'w', encoding='utf-8')\n",
    "        text_writer.write(text)\n",
    "        text_writer.close()\n",
    "\n",
    "\n",
    "#Files will be written with the fact_id + text type with the text string as the content\n",
    "#EX: fact_id 1000 = 1000-headline.txt, 1000-abstract.txt, 1000-lead_paragraph.txt\n",
    "id_dict = {t[1]:t[0] for t in ids}\n",
    "\n",
    "for k, v in big_text.items():\n",
    "    i = id_dict.get(k)\n",
    "    # #Headline\n",
    "    write_text_file('headline')\n",
    "    # # Abstact\n",
    "    write_text_file('abstract')\n",
    "    # # Lead Paragraph\n",
    "    write_text_file('lead_paragraph')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
