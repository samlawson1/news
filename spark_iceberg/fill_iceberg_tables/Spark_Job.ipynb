{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "#Python standard library\n",
    "from itertools import chain\n",
    "import json\n",
    "import os\n",
    "#Custom\n",
    "from nyt_article_search import JSONParse\n",
    "from spark_job_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load JSONs and transform in to lists of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists that the data from each json response will be added to\n",
    "article_ids = []\n",
    "fact_data = []\n",
    "author_data = []\n",
    "subject_data = []\n",
    "people_data = []\n",
    "org_data = []\n",
    "loc_data = []\n",
    "# Big text will be a dictionary where we'll add headline, lead paragraph,\n",
    "# abstract and web url\n",
    "big_text = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALABAMA', 'ALASKA', 'ARIZONA', 'ARKANSAS', 'CALIFORNIA', 'COLORADO', 'CONNECTICUT', 'DELAWARE', 'FLORIDA', 'GEORGIA', 'HAWAII', 'IDAHO', 'ILLINOIS', 'INDIANA', 'IOWA', 'KANSAS', 'KENTUCKY', 'LOUISIANA', 'MAINE', 'MARYLAND', 'MASSACHUSETTS', 'MICHIGAN', 'MINNESOTA', 'MISSISSIPPI', 'MISSOURI', 'MONTANA', 'NEBRASKA', 'NEVADA', 'NEW HAMPSHIRE', 'NEW JERSEY', 'NEW MEXICO', 'NEW YORK', 'NORTH CAROLINA', 'NORTH DAKOTA', 'OHIO', 'OKLAHOMA', 'OREGON', 'PENNSYLVANIA', 'RHODE ISLAND', 'SOUTH CAROLINA', 'SOUTH DAKOTA', 'TENNESSEE', 'TEXAS', 'UTAH', 'VERMONT', 'VIRGINIA', 'WASHINGTON', 'WEST VIRGINIA', 'WISCONSIN', 'WYOMING']\n"
     ]
    }
   ],
   "source": [
    "#Data folder where JSONs are located from api_call\n",
    "# - Should be 1 folder for every state\n",
    "data_folder = r'../../api_call/DATA'\n",
    "print(os.listdir(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "states = [f for f in os.listdir(data_folder)]\n",
    "for state in states:\n",
    "    state_folder = os.path.join(data_folder, state)\n",
    "    json_files = os.listdir(state_folder)\n",
    "    for file in json_files:\n",
    "        filepath = os.path.join(state_folder, file)\n",
    "        json_file = open(filepath, 'r')\n",
    "        data = json.load(json_file)\n",
    "        responses = data['response']['docs']\n",
    "        # Parse each json response in the json file\n",
    "        for respsonse in responses:\n",
    "            j = JSONParse(respsonse)\n",
    "            #Some JSON files share the same articles\n",
    "            #Only append if the article_id is not currently in the list\n",
    "            if j.article_id not in article_ids:\n",
    "                article_ids.append(j.article_id)\n",
    "                fact_data.append(j.get_article_facts())\n",
    "                extend_list(author_data, j.get_article_authors())\n",
    "                extend_list(subject_data, j.search_article_keywords('subject'))\n",
    "                extend_list(people_data, j.search_article_keywords('persons'))\n",
    "                extend_list(org_data, j.search_article_keywords('organizations'))\n",
    "                extend_list(loc_data, j.search_article_keywords('glocations'))\n",
    "                id_text_dict = {\n",
    "                    j.article_id:{\n",
    "                                    'headline':j.get_text('headline')[1],\n",
    "                                    'abstract':j.get_text('abstract')[1],\n",
    "                                    'lead_paragraph':j.get_text('lead_paragraph')[1],\n",
    "                                    'web_url':j.get_text('web_url')[1]\n",
    "                                }\n",
    "                    }\n",
    "                big_text.update(id_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iceberg_warehouse = os.environ.get('ICEBERG_WAREHOUSE')\n",
    "spark = SparkSession.builder.config(conf=spark_config(iceberg_warehouse)).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create spark dfs\n",
    "facts = spark.createDataFrame(fact_data, schema=get_table_headers('facts'))\n",
    "authors = spark.createDataFrame(author_data, schema=get_table_headers('authors'))\n",
    "subjects = spark.createDataFrame(subject_data, schema=get_table_headers('subjects'))\n",
    "people = spark.createDataFrame(people_data, schema=get_table_headers('subjects'))\n",
    "organizations = spark.createDataFrame(org_data, schema=get_table_headers('subjects'))\n",
    "locations = spark.createDataFrame(loc_data, schema=get_table_headers('subjects'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Primary Keys in tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to create an interger primary key for the article_ids\n",
    "ids = [(e + 1000, i) for e, i in enumerate(article_ids)]\n",
    "# Create spark df out of id list\n",
    "# Integer Primary Key is fact_id\n",
    "id_schema = StructType([\n",
    "    StructField('fact_id', IntegerType(), False),\n",
    "    StructField('article_id', StringType(), False)\n",
    "    ])\n",
    "\n",
    "### THIS SHOULD NOT CHANGE ANY FURTHER\n",
    "id_df = spark.createDataFrame(ids, schema=id_schema)\n",
    "\n",
    "\n",
    "\n",
    "#Merge the id_df dataframe into the existing frames\n",
    "# To put fact_id in all the other tables\n",
    "facts = id_df.join(facts, ['article_id'], how = 'inner').drop('article_id')\n",
    "authors = id_df.join(authors, ['article_id'], how = 'inner').drop('article_id')\n",
    "subjects = id_df.join(subjects, ['article_id'], how = 'inner').drop('article_id')\n",
    "people = id_df.join(people, ['article_id'], how = 'inner').drop('article_id')\n",
    "organizations = id_df.join(organizations, ['article_id'], how = 'inner').drop('article_id')\n",
    "locations = id_df.join(locations, ['article_id'], how = 'inner').drop('article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union the subjects, organizations, and locations dfs togehter\n",
    "places_and_things = subjects.union(organizations).union(locations).orderBy(['fact_id', 'rank'])\n",
    "places_and_things = create_primary_key(places_and_things, 'table_id', 'fact_id', 'rank')\n",
    "authors = create_primary_key(authors, 'table_id', 'fact_id', 'rank')\n",
    "people = create_primary_key(people, 'table_id', 'fact_id', 'rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Text standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in dimensional tables\n",
    "\n",
    "news_desk_df = spark.table('nyt.db.dim_news_desks')\n",
    "material_df = spark.table('nyt.db.dim_article_types')\n",
    "section_df = spark.table('nyt.db.dim_section_names')\n",
    "subject_id_df = spark.table('nyt.db.dim_subject_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize any text in the article_type, news_desk, and section_name\n",
    "#columns that aren't in the dimensinal tables\n",
    "#EX: National Desk vs National - should be National\n",
    "facts = standardize_text(facts, 'article_type', material_df)\n",
    "facts = standardize_text(facts, 'news_desk', news_desk_df)\n",
    "facts = standardize_text(facts, 'section_name', section_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the print_page column in facts df\n",
    "#Some print pages aren't numeric: 25A - A is already in print_section column\n",
    "#Create a list of unique page numbers \n",
    "page_rdd = facts.select(['print_page']).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "#include pages that aren't None\n",
    "pages = [p for p in set(page_rdd) if p != None]\n",
    "\n",
    "pages_cleaned = {}\n",
    "for page in pages:\n",
    "    if page != None:\n",
    "        try:\n",
    "            p = float(page)\n",
    "            p_dict = {page:p}\n",
    "            pages_cleaned.update(p_dict)\n",
    "        except:\n",
    "            if page == '':\n",
    "                p = None\n",
    "                p_dict = {page:p}\n",
    "                pages_cleaned.update(p_dict)\n",
    "            else:\n",
    "                p = page[:-1]\n",
    "                p = float(p)\n",
    "                p_dict = {page:p}\n",
    "                pages_cleaned.update(p_dict)\n",
    "    else:\n",
    "        pass\n",
    "#create mapping\n",
    "mapping = f.create_map([f.lit(x) for x in chain(*pages_cleaned.items())])\n",
    "\n",
    "#update the print_page column with mapping\n",
    "facts = facts.withColumn('print_page', mapping[facts['print_page']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN the people df create a standardized first name, middle name, and last name columns\n",
    "\n",
    "# Get the last name (left of comma) and upper case it\n",
    "people = people.withColumn('last_name', f.upper(f.substring_index('value', ',', 1)))\n",
    "# Get the first and middle name (right of comma) and upper case it\n",
    "people = people.withColumn('first_middle', f.upper(f.substring_index('value', ',', -1)))\n",
    "# Replace special characters in first and middle names with empty string,\n",
    "# But keep inner white space to separate middle name from first\n",
    "people = people.withColumn('first_middle', f.trim(f.regexp_replace('first_middle', '[^a-zA-Z ]', '')))\n",
    "# Identify name qualifiers - SR, JR, II, III, IV, V\n",
    "# EX - Dale Earnhardt vs Dale Earnhardt Jr\n",
    "qualifier_list = ['JR', 'SR', 'II', 'III', 'IV', 'V']\n",
    "qualifiers = people.where(f.substring_index('first_middle', ' ', -1).isin(qualifier_list)).select(['table_id', 'first_middle'])\n",
    "\n",
    "#Map replacement string for names with ending qualifiers\n",
    "name_replace = qualifiers.select('first_middle')\n",
    "name_list = name_replace.select(['first_middle']).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "name_dict = {name:name[0:name.rindex(' ')] for name in name_list}\n",
    "\n",
    "#update name_dict to include rest of names so mapping is correct\n",
    "non_qualifiers = people.where(~f.substring_index('first_middle', ' ', -1).isin(qualifier_list)).select(['table_id', 'first_middle'])\n",
    "non_qual_list = non_qualifiers.select(['first_middle']).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "for n in non_qual_list:\n",
    "    name_dict.update({n:n})\n",
    "\n",
    "\n",
    "#create mapping\n",
    "name_map = f.create_map([f.lit(x) for x in chain(*name_dict.items())])\n",
    "people = people.withColumn('first_middle', name_map[people['first_middle']])\n",
    "\n",
    "#Get middle name / initial\n",
    "people = people.withColumn('middle', \n",
    "    f.when(f.size(f.split(people['first_middle'], ' ', -1)) == 1, None).otherwise(f.substring_index(people['first_middle'], ' ', -1)))\n",
    "\n",
    "#Get first name\n",
    "people = people.withColumn('first_name', f.substring_index(people['first_middle'], ' ', 1))\n",
    "\n",
    "#Merge qualifiers back in as a separate column\n",
    "qualifiers = qualifiers.withColumn('qualifier', f.substring_index('first_middle', ' ', -1)).select('table_id', 'qualifier')\n",
    "people = people.join(qualifiers, ['table_id'], how = 'left').drop('value').drop('first_middle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors - rename columns to match people df\n",
    "rename_authors_cols = {\n",
    "    'lastname':'last_name',\n",
    "    'firstname':'first_name',\n",
    "    'middlename':'middle', \n",
    "    'rank':'role_rank',\n",
    "    'role':'author_role'\n",
    "}\n",
    "authors = authors.withColumnsRenamed(rename_authors_cols)\n",
    "\n",
    "#uppercase names\n",
    "authors = authors.withColumn('last_name', f.upper('last_name'))\n",
    "authors = authors.withColumn('first_name', f.upper('first_name'))\n",
    "authors = authors.withColumn('middle', f.upper('middle'))\n",
    "\n",
    "\n",
    "#people and plances_and_things - update columns\n",
    "peole_col_renamed = {\n",
    "    'rank':'subject_rank',\n",
    "    'major':'major_subject'\n",
    "}\n",
    "people = people.withColumnsRenamed(peole_col_renamed)\n",
    "\n",
    "places_and_things_col_renamed = {\n",
    "    'rank':'subject_rank',\n",
    "    'major':'major_subject',\n",
    "    'value':'subject'\n",
    "}\n",
    "places_and_things = places_and_things.withColumn('value', f.upper('value'))\n",
    "places_and_things = places_and_things.withColumnsRenamed(places_and_things_col_renamed)\n",
    "\n",
    "#Make major_role column true or false\n",
    "people = people.withColumn('major_subject', f.when(people['major_subject'] == 'Y', True).otherwise(False))\n",
    "places_and_things = places_and_things.withColumn('major_subject', f.when(places_and_things['major_subject'] == 'Y', True).otherwise(False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Use Dimensional Id's instead of text\n",
    "\n",
    "- Dimensional Tables:\n",
    "    - article_type_ids: material_df\n",
    "    - news_desk_ids: news_desk_df\n",
    "    - section_ids: section_df\n",
    "    - subjects_ids: create df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Sure Id's are ints\n",
    "material_df = material_df.withColumn('article_type_id', material_df['article_type_id'].cast(IntegerType()))\n",
    "news_desk_df = news_desk_df.withColumn('news_desk_id', news_desk_df['news_desk_id'].cast(IntegerType()))\n",
    "section_df = section_df.withColumn('section_name_id', section_df['section_name_id'].cast(IntegerType()))\n",
    "subject_id_df = subject_id_df.withColumn('subject_id', subject_id_df['subject_id'].cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "facts = facts.join(material_df, ['article_type'], how = 'left')\\\n",
    "        .join(news_desk_df, ['news_desk'], how = 'left')\\\n",
    "        .join(section_df, ['section_name'], how = 'left')\\\n",
    "        .drop(*('section_name', 'news_desk', 'article_type'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join subject id to places_and_things df\n",
    "subject_join = subject_id_df.withColumnRenamed('subject_name', 'name')\n",
    "places_and_things = places_and_things.join(subject_join, ['name'], how = 'left').drop('name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = facts.select(final_col_order('facts'))\n",
    "authors = authors.select(final_col_order('authors'))\n",
    "#add subject id to people\n",
    "people = people.withColumn('subject_id', f.lit(1))\n",
    "people = people.select(final_col_order('subject_people'))\n",
    "places_and_things = places_and_things.select(final_col_order('subject_others'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_check_dict = {}\n",
    "quality_checker = [\n",
    "            (id_df, 'article_id', 'article_ids'),\n",
    "            (facts, 'fact_id', 'facts'), \n",
    "            (authors, 'table_id', 'authors'),\n",
    "            (people, 'table_id', 'subject_people'),\n",
    "            (places_and_things, 'table_id', 'subject_others')\n",
    "            ]\n",
    "\n",
    "for table in quality_checker:\n",
    "    dup_check = duplicate_check(table[0], table[1], table[2])\n",
    "    quality_check_dict.update(dup_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #If the quality check passes, write the tables\n",
    "if all(quality_check_dict.values()) == True:\n",
    "    #Validate Schemas\n",
    "    facts = spark.createDataFrame(facts.rdd.collect(), schema=StructType(schema_validation('facts')))\n",
    "    authors = spark.createDataFrame(authors.rdd.collect(), schema=StructType(schema_validation('authors')))\n",
    "    people = spark.createDataFrame(people.rdd.collect(), schema=StructType(schema_validation('subject_people')))\n",
    "    places_and_things = spark.createDataFrame(places_and_things.rdd.collect(), schema=StructType(schema_validation('subject_others')))\n",
    "    id_df = spark.createDataFrame(id_df.rdd.collect(), schema=StructType(schema_validation('article_ids')))\n",
    "\n",
    "    #Sort partitioned tables\n",
    "    facts = facts.sort(facts['publication_date'])\n",
    "    places_and_things = places_and_things.sort(places_and_things['subject_id'])\n",
    "\n",
    "    #Write to iceberg tables\n",
    "    #No partitions\n",
    "    authors.writeTo('nyt.db.authors').append()\n",
    "    people.writeTo('nyt.db.subject_people').append()\n",
    "    id_df.writeTo('nyt.db.article_ids').append()\n",
    "    #Partitions\n",
    "    facts.writeTo('nyt.db.facts').partitionedBy('publication_date').append()\n",
    "    places_and_things.writeTo('nyt.db.subject_others').partitionedBy('subject_id').append()\n",
    "else:\n",
    "    table_failures = [k for k, v in quality_check_dict.items() if v == False]\n",
    "    tables_joined = ', '.join(table_failures)\n",
    "    message = f'The following tables failed the duplicate primary key quality check: {tables_joined}'\n",
    "    print(message)\n",
    "    raise Exception(message)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
