{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom\n",
    "from nyt_article_search import JSONParse\n",
    "from spark_job_functions import *\n",
    "#Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "#Normal stuff\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load JSONs and transform in to lists of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists that the data from each json response will be added to\n",
    "article_ids = []\n",
    "fact_data = []\n",
    "author_data = []\n",
    "subject_data = []\n",
    "people_data = []\n",
    "org_data = []\n",
    "loc_data = []\n",
    "# Big text will be a dictionary where we'll add headline, lead paragraph,\n",
    "# abstract and web url\n",
    "big_text = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALABAMA', 'ALASKA', 'ARIZONA', 'ARKANSAS', 'CALIFORNIA', 'COLORADO', 'CONNECTICUT', 'DELAWARE', 'FLORIDA', 'GEORGIA', 'HAWAII', 'IDAHO', 'ILLINOIS', 'INDIANA', 'IOWA', 'KANSAS', 'KENTUCKY', 'LOUISIANA', 'MAINE', 'MARYLAND', 'MASSACHUSETTS', 'MICHIGAN', 'MINNESOTA', 'MISSISSIPPI', 'MISSOURI', 'MONTANA', 'NEBRASKA', 'NEVADA', 'NEW HAMPSHIRE', 'NEW JERSEY', 'NEW MEXICO', 'NEW YORK', 'NORTH CAROLINA', 'NORTH DAKOTA', 'OHIO', 'OKLAHOMA', 'OREGON', 'PENNSYLVANIA', 'RHODE ISLAND', 'SOUTH CAROLINA', 'SOUTH DAKOTA', 'TENNESSEE', 'TEXAS', 'UTAH', 'VERMONT', 'VIRGINIA', 'WASHINGTON', 'WEST VIRGINIA', 'WISCONSIN', 'WYOMING']\n"
     ]
    }
   ],
   "source": [
    "#Data folder where JSONs are located from api_call\n",
    "# - Should be 1 folder for every state\n",
    "data_folder = r'../../api_call/DATA'\n",
    "print(os.listdir(data_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "states = [f for f in os.listdir(data_folder)]\n",
    "for state in states:\n",
    "    state_folder = os.path.join(data_folder, state)\n",
    "    json_files = os.listdir(state_folder)\n",
    "    for file in json_files:\n",
    "        filepath = os.path.join(state_folder, file)\n",
    "        json_file = open(filepath, 'r')\n",
    "        data = json.load(json_file)\n",
    "        responses = data['response']['docs']\n",
    "        # Parse each json response in the json file\n",
    "        for respsonse in responses:\n",
    "            j = JSONParse(respsonse)\n",
    "            article_ids.append(j.article_id)\n",
    "            fact_data.append(j.get_article_facts())\n",
    "            extend_list(author_data, j.get_article_authors())\n",
    "            extend_list(subject_data, j.search_article_keywords('subject'))\n",
    "            extend_list(people_data, j.search_article_keywords('persons'))\n",
    "            extend_list(org_data, j.search_article_keywords('organizations'))\n",
    "            extend_list(loc_data, j.search_article_keywords('glocations'))\n",
    "            id_text_dict = {\n",
    "                j.article_id:{\n",
    "                                'headline':j.get_text('headline')[1],\n",
    "                                'abstract':j.get_text('abstract')[1],\n",
    "                                'lead_paragraph':j.get_text('lead_paragraph')[1],\n",
    "                                'web_url':j.get_text('web_url')[1]\n",
    "                            }\n",
    "                }\n",
    "            big_text.update(id_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iceberg_warehouse = os.environ.get('ICEBERG_WAREHOUSE')\n",
    "spark = SparkSession.builder.config(conf=spark_config(iceberg_warehouse)).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create spark dfs\n",
    "facts = spark.createDataFrame(fact_data, schema=get_table_headers('facts'))\n",
    "authors = spark.createDataFrame(author_data, schema=get_table_headers('authors'))\n",
    "subjects = spark.createDataFrame(subject_data, schema=get_table_headers('subjects'))\n",
    "people = spark.createDataFrame(people_data, schema=get_table_headers('subjects'))\n",
    "organizations = spark.createDataFrame(org_data, schema=get_table_headers('subjects'))\n",
    "locations = spark.createDataFrame(loc_data, schema=get_table_headers('subjects'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Primary Keys in tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to create an interger primary key for the article_ids\n",
    "ids = [(e + 1000, i) for e, i in enumerate(article_ids)]\n",
    "# Create spark df out of id list\n",
    "# Integer Primary Key is fact_id\n",
    "id_schema = StructType([\n",
    "    StructField('fact_id', IntegerType(), False),\n",
    "    StructField('article_id', StringType(), False)\n",
    "    ])\n",
    "\n",
    "### THIS SHOULD NOT CHANGE ANY FURTHER\n",
    "id_df = spark.createDataFrame(ids, schema=id_schema)\n",
    "\n",
    "\n",
    "\n",
    "#Merge the id_df dataframe into the existing frames\n",
    "# To put fact_id in all the other tables\n",
    "facts = id_df.join(facts, ['article_id'], how = 'inner').drop('article_id')\n",
    "authors = id_df.join(authors, ['article_id'], how = 'inner').drop('article_id')\n",
    "subjects = id_df.join(subjects, ['article_id'], how = 'inner').drop('article_id')\n",
    "people = id_df.join(people, ['article_id'], how = 'inner').drop('article_id')\n",
    "organizations = id_df.join(organizations, ['article_id'], how = 'inner').drop('article_id')\n",
    "locations = id_df.join(locations, ['article_id'], how = 'inner').drop('article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union the subjects, organizations, and locations dfs togehter\n",
    "places_and_things = subjects.union(organizations).union(locations).orderBy(['fact_id', 'rank'])\n",
    "\n",
    "places_and_things = create_primary_key(places_and_things, 'table_id', 'fact_id', 'rank')\n",
    "authors = create_primary_key(authors, 'table_id', 'fact_id', 'rank')\n",
    "people = create_primary_key(people, 'table_id', 'fact_id', 'rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Text standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in dimensional tables\n",
    "\n",
    "news_desk_df = spark.table('nyt.db.dim_news_desks')\n",
    "material_df = spark.table('nyt.db.dim_article_types')\n",
    "section_df = spark.table('nyt.db.dim_section_names')\n",
    "subject_id_df = spark.table('nyt.db.dim_subject_ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+----------+--------------+-------------+-----------------+--------+----------+-------------+---------+------------+------------+\n",
      "|fact_id|publication_date|word_count|total_keywords|total_authors|words_in_headline|in_print|print_page|print_section|news_desk|section_name|article_type|\n",
      "+-------+----------------+----------+--------------+-------------+-----------------+--------+----------+-------------+---------+------------+------------+\n",
      "|   3330|      2006-08-18|       837|            12|            1|                9|    true|        12|            A| National|        U.S.|        News|\n",
      "|   2748|      2006-08-18|       837|            12|            1|                9|    true|        12|            A| National|        U.S.|        News|\n",
      "|   1027|      2006-08-18|       837|            12|            1|                9|    true|        12|            A| National|        U.S.|        News|\n",
      "|   5324|      2011-10-28|       452|            14|            1|                6|   false|      NULL|         NULL|     NULL|     Science|        News|\n",
      "|   4027|      2011-10-28|       452|            14|            1|                6|   false|      NULL|         NULL|     NULL|     Science|        News|\n",
      "+-------+----------------+----------+--------------+-------------+-----------------+--------+----------+-------------+---------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#standardize any text in the article_type, news_desk, and section_name\n",
    "#columns that aren't in the dimensinal tables\n",
    "#EX: National Desk vs National - should be National\n",
    "facts = standardize_text(facts, 'article_type', material_df)\n",
    "facts = standardize_text(facts, 'news_desk', news_desk_df)\n",
    "facts = standardize_text(facts, 'section_name', section_df)\n",
    "facts.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+----------+--------------+-------------+-----------------+--------+----------+-------------+---------+------------+------------+\n",
      "|fact_id|publication_date|word_count|total_keywords|total_authors|words_in_headline|in_print|print_page|print_section|news_desk|section_name|article_type|\n",
      "+-------+----------------+----------+--------------+-------------+-----------------+--------+----------+-------------+---------+------------+------------+\n",
      "|   3330|      2006-08-18|       837|            12|            1|                9|    true|      12.0|            A| National|        U.S.|        News|\n",
      "|   2748|      2006-08-18|       837|            12|            1|                9|    true|      12.0|            A| National|        U.S.|        News|\n",
      "|   1027|      2006-08-18|       837|            12|            1|                9|    true|      12.0|            A| National|        U.S.|        News|\n",
      "|   5324|      2011-10-28|       452|            14|            1|                6|   false|      NULL|         NULL|     NULL|     Science|        News|\n",
      "|   4027|      2011-10-28|       452|            14|            1|                6|   false|      NULL|         NULL|     NULL|     Science|        News|\n",
      "+-------+----------------+----------+--------------+-------------+-----------------+--------+----------+-------------+---------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#clean the print_page column in facts df\n",
    "#Some print pages aren't numeric: 25A - A is already in print_section column\n",
    "#Create a list of unique page numbers \n",
    "page_rdd = facts.select(['print_page']).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "#include pages that aren't None\n",
    "pages = [p for p in set(page_rdd) if p != None]\n",
    "\n",
    "pages_cleaned = {}\n",
    "for page in pages:\n",
    "    if page != None:\n",
    "        try:\n",
    "            p = float(page)\n",
    "            p_dict = {page:p}\n",
    "            pages_cleaned.update(p_dict)\n",
    "        except:\n",
    "            if page == '':\n",
    "                p = None\n",
    "                p_dict = {page:p}\n",
    "                pages_cleaned.update(p_dict)\n",
    "            else:\n",
    "                p = page[:-1]\n",
    "                p = float(p)\n",
    "                p_dict = {page:p}\n",
    "                pages_cleaned.update(p_dict)\n",
    "    else:\n",
    "        pass\n",
    "#create mapping\n",
    "mapping = f.create_map([f.lit(x) for x in chain(*pages_cleaned.items())])\n",
    "\n",
    "#update the print_page column with mapping\n",
    "facts = facts.withColumn('print_page', mapping[facts['print_page']])\n",
    "facts.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN the people df create a standardized first name, middle name, and last name columns\n",
    "\n",
    "# Get the last name (left of comma) and upper case it\n",
    "people = people.withColumn('last_name', f.upper(f.substring_index('value', ',', 1)))\n",
    "# Get the first and middle name (right of comma) and upper case it\n",
    "people = people.withColumn('first_middle', f.upper(f.substring_index('value', ',', -1)))\n",
    "# Replace special characters in first and middle names with empty string,\n",
    "# But keep inner white space to separate middle name from first\n",
    "people = people.withColumn('first_middle', f.trim(f.regexp_replace('first_middle', '[^a-zA-Z ]', '')))\n",
    "# Identify name qualifiers - SR, JR, II, III, IV, V\n",
    "# EX - Dale Earnhardt vs Dale Earnhardt Jr\n",
    "qualifier_list = ['JR', 'SR', 'II', 'III', 'IV', 'V']\n",
    "qualifiers = people.where(f.substring_index('first_middle', ' ', -1).isin(qualifier_list)).select(['table_id', 'first_middle'])\n",
    "\n",
    "#Map replacement string for names with ending qualifiers\n",
    "name_replace = qualifiers.select('first_middle')\n",
    "name_list = name_replace.select(['first_middle']).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "name_dict = {name:name[0:name.rindex(' ')] for name in name_list}\n",
    "\n",
    "#update name_dict to include rest of names so mapping is correct\n",
    "non_qualifiers = people.where(~f.substring_index('first_middle', ' ', -1).isin(qualifier_list)).select(['table_id', 'first_middle'])\n",
    "non_qual_list = non_qualifiers.select(['first_middle']).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "for n in non_qual_list:\n",
    "    name_dict.update({n:n})\n",
    "\n",
    "\n",
    "#create mapping\n",
    "name_map = f.create_map([f.lit(x) for x in chain(*name_dict.items())])\n",
    "people = people.withColumn('first_middle', name_map[people['first_middle']])\n",
    "\n",
    "#Get middle name / initial\n",
    "people = people.withColumn('middle', \n",
    "    f.when(f.size(f.split(people['first_middle'], ' ', -1)) == 1, None).otherwise(f.substring_index(people['first_middle'], ' ', -1)))\n",
    "\n",
    "#Get first name\n",
    "people = people.withColumn('first_name', f.substring_index(people['first_middle'], ' ', 1))\n",
    "\n",
    "#Merge qualifiers back in as a separate column\n",
    "qualifiers = qualifiers.withColumn('qualifier', f.substring_index('first_middle', ' ', -1)).select('table_id', 'qualifier')\n",
    "people = people.join(qualifiers, ['table_id'], how = 'left').drop('value').drop('first_middle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors - rename columns to match people df\n",
    "rename_authors_cols = {\n",
    "    'lastname':'last_name',\n",
    "    'firstname':'first_name',\n",
    "    'middlename':'middle', \n",
    "    'rank':'role_rank',\n",
    "    'role':'author_role'\n",
    "}\n",
    "authors = authors.withColumnsRenamed(rename_authors_cols)\n",
    "\n",
    "#uppercase names\n",
    "authors = authors.withColumn('last_name', f.upper('last_name'))\n",
    "authors = authors.withColumn('first_name', f.upper('first_name'))\n",
    "authors = authors.withColumn('middle', f.upper('middle'))\n",
    "\n",
    "\n",
    "#people and plances_and_things - update columns\n",
    "peole_col_renamed = {\n",
    "    'rank':'subject_rank',\n",
    "    'major':'major_subject'\n",
    "}\n",
    "people = people.withColumnsRenamed(peole_col_renamed)\n",
    "\n",
    "places_and_things_col_renamed = {\n",
    "    'rank':'subject_rank',\n",
    "    'major':'major_subject',\n",
    "    'value':'subject'\n",
    "}\n",
    "places_and_things = places_and_things.withColumn('value', f.upper('value'))\n",
    "places_and_things = places_and_things.withColumnsRenamed(places_and_things_col_renamed)\n",
    "\n",
    "#Make major_role column true or false\n",
    "people = people.withColumn('major_subject', f.when(people['major_subject'] == 'Y', True).otherwise(False))\n",
    "places_and_things = places_and_things.withColumn('major_subject', f.when(places_and_things['major_subject'] == 'Y', True).otherwise(False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Use Dimensional Id's instead of text\n",
    "\n",
    "- Dimensional Tables:\n",
    "    - article_type_ids: material_df\n",
    "    - news_desk_ids: news_desk_df\n",
    "    - section_ids: section_df\n",
    "    - subjects_ids: create df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Sure Id's are ints\n",
    "material_df = material_df.withColumn('article_type_id', material_df['article_type_id'].cast(IntegerType()))\n",
    "news_desk_df = news_desk_df.withColumn('news_desk_id', news_desk_df['news_desk_id'].cast(IntegerType()))\n",
    "section_df = section_df.withColumn('section_name_id', section_df['section_name_id'].cast(IntegerType()))\n",
    "subject_id_df = subject_id_df.withColumn('subject_id', subject_id_df['subject_id'].cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "facts = facts.join(material_df, ['article_type'], how = 'left')\\\n",
    "        .join(news_desk_df, ['news_desk'], how = 'left')\\\n",
    "        .join(section_df, ['section_name'], how = 'left')\\\n",
    "        .drop(*('section_name', 'news_desk', 'article_type'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join subject id to places_and_things df\n",
    "subject_join = subject_id_df.withColumnRenamed('subject_name', 'name')\n",
    "places_and_things = places_and_things.join(subject_join, ['name'], how = 'left').drop('name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = facts.select(final_col_order('facts'))\n",
    "authors = authors.select(final_col_order('authors'))\n",
    "#add subject id to people\n",
    "people = people.withColumn('subject_id', f.lit(1))\n",
    "people = people.select(final_col_order('subject_people'))\n",
    "places_and_things = places_and_things.select(final_col_order('subject_others'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate Schemas\n",
    "facts = spark.createDataFrame(facts.rdd.collect(), schema=StructType(schema_validation('facts')))\n",
    "authors = spark.createDataFrame(authors.rdd.collect(), schema=StructType(schema_validation('authors')))\n",
    "people = spark.createDataFrame(people.rdd.collect(), schema=StructType(schema_validation('subject_people')))\n",
    "places_and_things = spark.createDataFrame(places_and_things.rdd.collect(), schema=StructType(schema_validation('subject_others')))\n",
    "id_df = spark.createDataFrame(id_df.rdd.collect(), schema=StructType(schema_validation('article_ids')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Write to iceberg tables\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# facts.writeTo('nyt.db.facts').append()\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# authors.writeTo('nyt.db.authors').append()\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# people.inserwriteTotInto('nyt.db.subject_people').append()\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# places_and_things.writeTo('nyt.db.subject_others').append()\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mid_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnyt.db.article_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mappend()\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\pyspark\\sql\\dataframe.py:5717\u001b[0m, in \u001b[0;36mDataFrame.writeTo\u001b[1;34m(self, table)\u001b[0m\n\u001b[0;32m   5685\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwriteTo\u001b[39m(\u001b[38;5;28mself\u001b[39m, table: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrameWriterV2:\n\u001b[0;32m   5686\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5687\u001b[0m \u001b[38;5;124;03m    Create a write configuration builder for v2 sources.\u001b[39;00m\n\u001b[0;32m   5688\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5715\u001b[0m \u001b[38;5;124;03m    ... ).partitionedBy(\"col\").createOrReplace()\u001b[39;00m\n\u001b[0;32m   5716\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameWriterV2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\pyspark\\sql\\readwriter.py:2001\u001b[0m, in \u001b[0;36mDataFrameWriterV2.__init__\u001b[1;34m(self, df, table)\u001b[0m\n\u001b[0;32m   1999\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m df\n\u001b[0;32m   2000\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msparkSession\n\u001b[1;32m-> 2001\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwriter \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32md:\\Projects\\news\\lib\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "#Write to iceberg tables\n",
    "# facts.writeTo('nyt.db.facts').append()\n",
    "# authors.writeTo('nyt.db.authors').append()\n",
    "# people.inserwriteTotInto('nyt.db.subject_people').append()\n",
    "# places_and_things.writeTo('nyt.db.subject_others').append()\n",
    "# id_df.writeTo('nyt.db.article_ids').append()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
