{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom\n",
    "from nyt_article_search import JSONParse\n",
    "from spark_job_functions import *\n",
    "#Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "#Normal stuff\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load JSONs and transform in to lists of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists that the data from each json response will be added to\n",
    "article_ids = []\n",
    "fact_data = []\n",
    "author_data = []\n",
    "subject_data = []\n",
    "people_data = []\n",
    "org_data = []\n",
    "loc_data = []\n",
    "# Big text will be a dictionary where we'll add headline, lead paragraph,\n",
    "# abstract and web url\n",
    "big_text = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'DATA'\n",
    "state = 'TEXAS'\n",
    "# states = [f for f in os.listdir(data_folder) if f != '_Archive']\n",
    "state_folder = os.path.join(data_folder, state)\n",
    "json_files = os.listdir(state_folder)\n",
    "for file in json_files:\n",
    "    filepath = os.path.join(state_folder, file)\n",
    "    json_file = open(filepath, 'r')\n",
    "    data = json.load(json_file)\n",
    "    responses = data['response']['docs']\n",
    "    # Parse each json response in the json file\n",
    "    for respsonse in responses:\n",
    "        j = JSONParse(respsonse)\n",
    "        article_ids.append(j.article_id)\n",
    "        fact_data.append(j.get_article_facts())\n",
    "        extend_list(author_data, j.get_article_authors())\n",
    "        extend_list(subject_data, j.search_article_keywords('subject'))\n",
    "        extend_list(people_data, j.search_article_keywords('persons'))\n",
    "        extend_list(org_data, j.search_article_keywords('organizations'))\n",
    "        extend_list(loc_data, j.search_article_keywords('glocations'))\n",
    "        id_text_dict = {\n",
    "            j.article_id:{\n",
    "                            'headline':j.get_text('headline')[1],\n",
    "                            'abstract':j.get_text('abstract')[1],\n",
    "                            'lead_paragraph':j.get_text('lead_paragraph')[1],\n",
    "                            'web_url':j.get_text('web_url')[1]\n",
    "                        }\n",
    "            }\n",
    "        big_text.update(id_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName('NYT_JSON_ETL').\\\n",
    "        master('local[1]').\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create spark dfs\n",
    "facts = spark.createDataFrame(fact_data, schema=get_table_headers('facts'))\n",
    "authors = spark.createDataFrame(author_data, schema=get_table_headers('authors'))\n",
    "subjects = spark.createDataFrame(subject_data, schema=get_table_headers('subjects'))\n",
    "people = spark.createDataFrame(people_data, schema=get_table_headers('subjects'))\n",
    "organizations = spark.createDataFrame(org_data, schema=get_table_headers('subjects'))\n",
    "locations = spark.createDataFrame(loc_data, schema=get_table_headers('subjects'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Primary Keys in tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to create an interger primary key for the article_ids\n",
    "ids = [(e + 1000, i) for e, i in enumerate(article_ids)]\n",
    "# Create spark df out of id list\n",
    "# Integer Primary Key is fact_id\n",
    "id_schema = StructType([\n",
    "    StructField('fact_id', IntegerType(), False),\n",
    "    StructField('article_id', StringType(), False)\n",
    "    ])\n",
    "\n",
    "### THIS SHOULD NOT CHANGE ANY FURTHER\n",
    "id_df = spark.createDataFrame(ids, schema=id_schema)\n",
    "\n",
    "\n",
    "\n",
    "#Merge the id_df dataframe into the existing frames\n",
    "# To put fact_id in all the other tables\n",
    "facts = id_df.join(facts, ['article_id'], how = 'inner').drop('article_id')\n",
    "authors = id_df.join(authors, ['article_id'], how = 'inner').drop('article_id')\n",
    "subjects = id_df.join(subjects, ['article_id'], how = 'inner').drop('article_id')\n",
    "people = id_df.join(people, ['article_id'], how = 'inner').drop('article_id')\n",
    "organizations = id_df.join(organizations, ['article_id'], how = 'inner').drop('article_id')\n",
    "locations = id_df.join(locations, ['article_id'], how = 'inner').drop('article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union the subjects, organizations, and locations dfs togehter\n",
    "places_and_things = subjects.union(organizations).union(locations).orderBy(['fact_id', 'rank'])\n",
    "\n",
    "places_and_things = create_primary_key(places_and_things, 'table_id', 'fact_id', 'rank')\n",
    "authors = create_primary_key(authors, 'table_id', 'fact_id', 'rank')\n",
    "people = create_primary_key(people, 'table_id', 'fact_id', 'rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Text standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dimensional news desk, section name, and article type files\n",
    "dim_folder = os.path.join('DATA', 'DIM_TABLES')\n",
    "dim_news_desk = os.path.join(dim_folder, 'news_desks.csv')\n",
    "dim_types = os.path.join(dim_folder, 'article_types.csv')\n",
    "dim_sections = os.path.join(dim_folder, 'section_names.csv')\n",
    "\n",
    "news_desk_df = spark.read.option('header', True).csv(dim_news_desk)\n",
    "material_df = spark.read.option('header', True).csv(dim_types)\n",
    "section_df = spark.read.option('header', True).csv(dim_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize any text in the article_type, news_desk, and section_name\n",
    "#columns that aren't in the dimensinal tables\n",
    "#EX: National Desk vs National - should be National\n",
    "facts = standardize_text(facts, 'article_type', material_df)\n",
    "facts = standardize_text(facts, 'news_desk', news_desk_df)\n",
    "facts = standardize_text(facts, 'section_name', section_df)\n",
    "facts.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the print_page column in facts df\n",
    "#Some print pages aren't numeric: 25A - A is already in print_section column\n",
    "#Create a list of unique page numbers \n",
    "page_rdd = facts.select(['print_page']).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "#include pages that aren't None\n",
    "pages = [p for p in set(page_rdd) if p != None]\n",
    "\n",
    "pages_cleaned = {}\n",
    "for page in pages:\n",
    "    if page != None:\n",
    "        try:\n",
    "            p = float(page)\n",
    "            p_dict = {page:p}\n",
    "            pages_cleaned.update(p_dict)\n",
    "        except:\n",
    "            p = page[:-1]\n",
    "            p = float(p)\n",
    "            p_dict = {page:p}\n",
    "            pages_cleaned.update(p_dict)\n",
    "    else:\n",
    "        pass\n",
    "#create mapping\n",
    "mapping = f.create_map([f.lit(x) for x in chain(*pages_cleaned.items())])\n",
    "\n",
    "#update the print_page column with mapping\n",
    "facts = facts.withColumn('print_page', mapping[facts['print_page']])\n",
    "facts.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN the people df create a standardized first name, middle name, and last name columns\n",
    "\n",
    "# Get the last name (left of comma) and upper case it\n",
    "people = people.withColumn('last_name', f.upper(f.substring_index('value', ',', 1)))\n",
    "# Get the first and middle name (right of comma) and upper case it\n",
    "people = people.withColumn('first_middle', f.upper(f.substring_index('value', ',', -1)))\n",
    "# Replace special characters in first and middle names with empty string,\n",
    "# But keep inner white space to separate middle name from first\n",
    "people = people.withColumn('first_middle', f.trim(f.regexp_replace('first_middle', '[^a-zA-Z ]', '')))\n",
    "# Identify name qualifiers - SR, JR, II, III, IV, V\n",
    "# EX - Dale Earnhardt vs Dale Earnhardt Jr\n",
    "qualifier_list = ['JR', 'SR', 'II', 'III', 'IV', 'V']\n",
    "qualifiers = people.where(f.substring_index('first_middle', ' ', -1).isin(qualifier_list)).select(['table_id', 'first_middle'])\n",
    "\n",
    "#Map replacement string for names with ending qualifiers\n",
    "name_replace = qualifiers.select('first_middle')\n",
    "name_list = name_replace.select(['first_middle']).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "name_dict = {name:name[0:name.rindex(' ')] for name in name_list}\n",
    "\n",
    "#update name_dict to include rest of names so mapping is correct\n",
    "non_qualifiers = people.where(~f.substring_index('first_middle', ' ', -1).isin(qualifier_list)).select(['table_id', 'first_middle'])\n",
    "non_qual_list = non_qualifiers.select(['first_middle']).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "for n in non_qual_list:\n",
    "    name_dict.update({n:n})\n",
    "\n",
    "\n",
    "#create mapping\n",
    "name_map = f.create_map([f.lit(x) for x in chain(*name_dict.items())])\n",
    "people = people.withColumn('first_middle', name_map[people['first_middle']])\n",
    "\n",
    "#Get middle name / initial\n",
    "people = people.withColumn('middle', \n",
    "    f.when(f.size(f.split(people['first_middle'], ' ', -1)) == 1, None).otherwise(f.substring_index(people['first_middle'], ' ', -1)))\n",
    "\n",
    "#Get first name\n",
    "people = people.withColumn('first_name', f.substring_index(people['first_middle'], ' ', 1))\n",
    "\n",
    "#Merge qualifiers back in as a separate column\n",
    "qualifiers = qualifiers.withColumn('qualifier', f.substring_index('first_middle', ' ', -1)).select('table_id', 'qualifier')\n",
    "people = people.join(qualifiers, ['table_id'], how = 'left').drop('value').drop('first_middle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors - rename columns to match people df\n",
    "rename_authors_cols = {\n",
    "    'lastname':'last_name',\n",
    "    'firstname':'first_name',\n",
    "    'middlename':'middle', \n",
    "    'rank':'role_rank',\n",
    "    'role':'author_role'\n",
    "}\n",
    "authors = authors.withColumnsRenamed(rename_authors_cols)\n",
    "\n",
    "#uppercase names\n",
    "authors = authors.withColumn('last_name', f.upper('last_name'))\n",
    "authors = authors.withColumn('first_name', f.upper('first_name'))\n",
    "authors = authors.withColumn('middle', f.upper('middle'))\n",
    "\n",
    "\n",
    "#people and plances_and_things - update columns\n",
    "peole_col_renamed = {\n",
    "    'rank':'subject_rank',\n",
    "    'major':'major_subject'\n",
    "}\n",
    "people = people.withColumnsRenamed(peole_col_renamed)\n",
    "\n",
    "places_and_things_col_renamed = {\n",
    "    'rank':'subject_rank',\n",
    "    'major':'major_subject',\n",
    "    'value':'subject'\n",
    "}\n",
    "places_and_things = places_and_things.withColumn('value', f.upper('value'))\n",
    "places_and_things = places_and_things.withColumnsRenamed(places_and_things_col_renamed)\n",
    "\n",
    "#Make major_role column true or false\n",
    "people = people.withColumn('major_subject', f.when(people['major_subject'] == 'Y', True).otherwise(False))\n",
    "places_and_things = places_and_things.withColumn('major_subject', f.when(places_and_things['major_subject'] == 'Y', True).otherwise(False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Use Dimensional Id's instead of text\n",
    "\n",
    "- Dimensional Tables:\n",
    "    - article_type_ids: material_df\n",
    "    - news_desk_ids: news_desk_df\n",
    "    - section_ids: section_df\n",
    "    - subjects_ids: create df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Sure Id's are ints\n",
    "material_df = material_df.withColumn('article_type_id', material_df['article_type_id'].cast(IntegerType()))\n",
    "news_desk_df = news_desk_df.withColumn('news_desk_id', news_desk_df['news_desk_id'].cast(IntegerType()))\n",
    "section_df = section_df.withColumn('section_name_id', section_df['section_name_id'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "facts = facts.join(material_df, ['article_type'], how = 'left')\\\n",
    "        .join(news_desk_df, ['news_desk'], how = 'left')\\\n",
    "        .join(section_df, ['section_name'], how = 'left')\\\n",
    "        .drop(*('section_name', 'news_desk', 'article_type'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids = [\n",
    "    (1, 'persons'),\n",
    "    (2, 'organizations'),\n",
    "    (3, 'glocations'),\n",
    "    (4, 'subject')\n",
    "]\n",
    "subject_id_df = spark.createDataFrame(subject_ids, schema=['subject_id', 'name'])\n",
    "\n",
    "people = people.join(subject_id_df, ['name'], how = 'left').drop('name')\n",
    "places_and_things = places_and_things.join(subject_id_df, ['name'], how = 'left').drop('name')\n",
    "subject_id_df = subject_id_df.withColumnRenamed('name', 'subject_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = facts.select(final_col_order('facts'))\n",
    "authors = authors.select(final_col_order('authors'))\n",
    "people = people.select(final_col_order('subject_people'))\n",
    "places_and_things = places_and_things.select(final_col_order('subject_others'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate Schemas\n",
    "facts = spark.createDataFrame(facts.rdd.collect(), schema=StructType(schema_validation('facts')))\n",
    "authors = spark.createDataFrame(authors.rdd.collect(), schema=StructType(schema_validation('authors')))\n",
    "people = spark.createDataFrame(people.rdd.collect(), schema=StructType(schema_validation('subject_people')))\n",
    "places_and_things = spark.createDataFrame(places_and_things.rdd.collect(), schema=StructType(schema_validation('subject_others')))\n",
    "id_df = spark.createDataFrame(id_df.rdd.collect(), schema=StructType(schema_validation('article_ids')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
